# Remote Sensing Data and Corrections {#remote-sensing-data-corrections}

## Summary {#sec-3-summary}

This week we delved deeper into remote sensing topics particularly data corrections, joining and enhancements. Lots of new terms were raised and some of the concepts were challenging to understand so it's easiest to summarise these terms by building a @tbl-remote-sensing-glossary. A lot of these terms could have entire lectures dedicated to them so I attempt to put these into a simple terms as possible.

```{r include=FALSE}
library(tidyverse)
library(knitr)
library(here)
library(kableExtra)

#read in data
glos <- readxl::read_excel(here("tables", "rs_glossary.xlsx"))%>% 
  knitr::kable(booktabs = TRUE)%>% 
  kable_styling(position = "center", full_width = T)%>%
    row_spec(.,
  row=0,
  bold = TRUE)
```

```{r echo=FALSE}
#| label: tbl-remote-sensing-glossary
#| tbl-cap: "Glossary of remote sensing terminology. <br> **Sources:** @gisgeography2015. @esri2025, @usgs2025, @jaadi2024, @irons1981, @wen2024"
glos
```

A key point raised throughout the lecture was that regression plays a key part in many of the corrections that we looked into. This links back to (@sec-1-application) discussion and the regression based methods used by @mandanici2016. I initially struggled to fully understand this methodology fully so this lecture helped to consolidate those comparisons by clearly explaining the important role that regression plays in remote sensing.

Once again we have near endless options to analyse further but this week I will focus on atmospheric correction by comparing against radiometric normalisation. This specifically looks at the differences and accuracy of **relative** and **absolute** correction.

## Applications - Effectiveness of relative and absolute correction {#sec-3-application}

This is an interesting area to investigate because the availability of inputs for absolute atmospheric correction is often generalized over a specified area or season, which can lead to accuracy issues. Both @hu2011 and @bernardo2016 investigate the impact of this generalisation. Although there are differences between the two methodologies, let’s first examine their similarities:

-   compares using Landsat imagery
-   compares a single study area using time series images
-   uses FLAASH for absolute atmospheric correction
-   uses multivariate alternative detection (MAD) for relative radiometric normalisation

### Absolute Vs Relative methods and results

@bernardo2016 investigated the reflectance errors comparing by assessing root mean squared error (RMSE) and the mean absolute percent error (MAPE). A comparison was made against the FLAASH (absolute method) against IRMAD - iteratively reweighted multivariate alteration detection (relative method) correction methods. The results in @tbl-reflectance-errors clearly show greater levels in inaccuracy from the absolute method (FLAASH) in both methods of comparison across nearly all bands.

```{r include=FALSE}
#read in data
referrors <- read_csv(here("tables","flaash_irmad_comparison.csv"), locale = locale(encoding = "UTF-8"))%>% 
  knitr::kable(booktabs = TRUE)%>% 
  kable_styling(position = "center", full_width = T)%>%
    column_spec(c(2,3), background = "#F5F5F5") %>%
    column_spec(c(4,5), background = "#D3D3D3") %>%
    row_spec(., row=0, bold = TRUE)
```

```{r echo=FALSE}
#| label: tbl-reflectance-errors
#| tbl-cap: "Reflectance errors from FLAASH and IRMAD corrections **[@bernardo2016]**"
referrors
```

@hu2011 used MAD, over @bernardo2016's technique of IRMAD, which provides a more basic detection of change as it is a one-pass approach and does not iteratively refine the results, also IRMAD are much less noisy than MAD (@nielsen2005, @nielsen2007). There is also a third approach, regularised IRMAD which can perform even better but this was not part of the studies being compared.

@hu2011 used regression to assess correction differences, with results comparable to those of @bernardo2016. Both methods reduce radiometric distortion, but relative normalization corrects it more effectively, as shown by the lower standard deviations of relative normalization versus atmospheric correction across all bands (@tbl-std-comparison).

```{r include=FALSE}
#read in data
sdcomp <- read_csv(here("tables","sd_rel_ab_comparison.csv"), locale = locale(encoding = "UTF-8"))%>% 
  knitr::kable(booktabs = TRUE)%>% 
  kable_styling(position = "center", full_width = T)%>%
    row_spec(., row=0, bold = TRUE)
```

```{r echo=FALSE}
#| label: tbl-std-comparison
#| tbl-cap: "Standard Deviation of the difference in each band for relative normalisation against atmospheric correction"
sdcomp
```

### Review

Although the papers mention high percentage errors in certain bands for absolute corrections, these may be specific to the study location and the assumptions made during the analysis. With so many variables at play, it’s unlikely that any study could comprehensively address them all. Ideally it would be possible to say exactly what corrections/enhancements to apply to any image to provide the optimal output. However that is just not possible, considering variables including atmospheric conditions, topological factors, sensor differences, then we can begin to see why there is no single set of tools for analysis.

This is evident in both papers and also seen in other papers such as @mandanici2016 and @nasiri2022. It could appear that the scope is quite limited as they focus on very small samples for their analysis (previously discussed in week 1). The technical challenge lies in the immense computational power required to process larger samples, making it both restrictive and time-consuming.

## Reflection {#sec-3-refection}

It became clear early on that the topics discussed this week could easily fill entire lectures. However as I delved deeper into the concepts I could really start to see the benefit corrections and enhancements have as part of wider studies, as long as the impact of these have on results is considered especially regarding the specific use case that is being analysed.

I'm interested in putting this into practice through the Earth Observation Data Hub that Camden will soon have access to. I want to explore the different data types and can now understand the differences between products and in particular what information can be derived from available bands. Most importantly I will no longer take results at face value and will use this knowledge to be critical and actually think how results can be improved. This is a vast improvement on previous EO work completed which involved blindly following guides to get results that seems similar.

One final note, this week's lecture and learning diary improved my overall understanding of the challenges with remote sensing data, but I'm glad that the processing is built into existing packages or through Analysis Ready Data.
