# Remote Sensing Data and Corrections {#remote-sensing-data-corrections}

## Summary {#sec-3-summary}

This week we delved deeper into remote sensing topics particularly data corrections, joining and enhancements. Lots of new terms were raised and some of the concepts were challenging to understand so it's easiest to summarise these terms by building a @tbl-remote-sensing-glossary. A lot of these terms could have entire lectures dedicated to them so I attempt to put these into a simple terms as possible.

*sources need to be added to the caption below*

```{r include=FALSE}
library(tidyverse)
library(knitr)
library(here)
library(kableExtra)

#read in data
glos <- readxl::read_excel(here("tables", "rs_glossary.xlsx"))%>% 
  knitr::kable(booktabs = TRUE)%>% 
  kable_styling(position = "center", full_width = T)%>%
  # any specifc row changes you want
    row_spec(.,
  row=0,
  bold = TRUE)
```

```{r echo=FALSE}
#| label: tbl-remote-sensing-glossary
#| tbl-cap: "Glossary of remote sensing terminology. **Sources: **"
glos
```

A key point raised throughout the lecture was that regression plays a key part in many of the corrections that we looked into. This links back to (@sec-1-application) discussion and the regression based methods used by @mandanici2016. I initially struggled to fully understand this methodology fully so this lecture helped to consolidate those comparisons by clearly explaining the important role that regression plays in remote sensing.

Once again we have near endless options to analyse further but this week I will focus on atmospheric correction by comparing against radiometric normalisation. This specifically looks at the differences and accuracy of **relative** and **absolute** correction.

## Applications - Effectiveness of relative and absolute correction {#sec-3-application}

The is an interesting area to investigate is because the availability of the inputs required for absolute atmospheric correction is often generalised over an specified area or season potentially leading to accuracy issues. Both @hu2011 and @bernardo2016 investigation the impact of this generalisation, whilst there are differences between the two methodologies which will be discussed later but first lets look at the similarities:

-   compares using Landsat imagery
-   compares a single study area using time series images
-   uses FLAASH for absolute atmospheric correction
-   uses multivariate alternative detection (MAD) for relative radiometric normalisation

### Absolute Vs Relative methods and results

@bernardo2016 investigated the reflectance errors comparing by assessing root mean squared error (RMSE) and the mean absolute percent error (MAPE). A comparison was made against the FLAASH (absolute method) against IRMAD - iteratively reweighted multivariate alteration detection (relative method) correction methods. The results in @tbl-reflectance-errors clearly show greater levels in inaccuracy from the absolute method (FLAASH) in both methods of comparison across nearly all bands.

```{r include=FALSE}
#read in data
referrors <- read_csv(here("tables","flaash_irmad_comparison.csv"), locale = locale(encoding = "UTF-8"))%>% 
  knitr::kable(booktabs = TRUE)%>% 
  kable_styling(position = "center", full_width = T)%>%
    column_spec(c(2,3), background = "#F5F5F5") %>%
    column_spec(c(4,5), background = "#D3D3D3") %>%
    row_spec(., row=0, bold = TRUE)
```

```{r echo=FALSE}
#| label: tbl-reflectance-errors
#| tbl-cap: "Reflectance errors from FLAASH and IRMAD corrections **[@bernardo2016]**"
referrors
```

@hu2011 used MAD, over @bernardo2016's technique of IRMAD, which provides a more basic detection of change as it is a one-pass approach and does not iteratively refine the results, also IRMAD are much less noisy than MAD(@nielsen2005, @nielsen2007). There is also a third approach, regularised IRMAD which can perform even better but this was not part of the studies being compared.

@hu2011 used regression to assess the difference between corrections and the findings can be reviewed through the standard deviation results of each method. Generally the results are comparable to @bernardo2016 with both methods reducing radiometric distortion, however relative normalisation corrects it more effectively. This is shown by the lower standard deviations of relative normalisation versus atmospheric correction across all bands (@tbl-std-comparison).

```{r include=FALSE}
#read in data
sdcomp <- read_csv(here("tables","sd_rel_ab_comparison.csv"), locale = locale(encoding = "UTF-8"))%>% 
  knitr::kable(booktabs = TRUE)%>% 
  kable_styling(position = "center", full_width = T)%>%
    row_spec(., row=0, bold = TRUE)
```

```{r echo=FALSE}
#| label: tbl-std-comparison
#| tbl-cap: "Standard Deviation of the difference in each band for relative normalisation against atmospheric correction"
sdcomp
```

### Review

Whilst the papers talk about absolute corrections having high percentage error in certain bands this could be specific to the location of the study and any assumptions made throughout. There are so many variables that it would be highly improbable to produce a study that could feasibly review all of these aspects. Ideally it would be possible to say exactly what corrections/enhancements to apply to any image to provide the optimal output. However that is just not possible, considering variables including atmospheric conditions, topological factors, sensor differences, then we can begin to see why there is no single set of tools for analysis.

This is evident in both papers and also seen in other papers such as @mandanici2016 and @nasiri2022. It could appear that the scope is quite limited as they focus on very small samples for their analysis (previously discussed in week 1). However if we consider this from a technical point of view then simply the amount of computational power required to process much larger samples is very restrictive and time-consuming.

## Reflection {#sec-3-refection}

It was clear early on that the topics discussed this week could have lectures dedicated to them. However as I delved deeper into the concepts I could really start to see the benefit corrections and enhancements have as part of wider studies, as long as the impact of these have on results is considered especially regarding the specific use case that is being analysed.

I'm interested in putting this into practice through the Earth Observation Data Hub that Camden will soon have access to. I want to explore the different data types and can now understand the differences between products and in particular what information can be derived from available bands. Most importantly I will no longer take results at face value and will use this knowledge to be critical and actually think how results can be improved.  This is a vast improvement on previous EO work completed which involved blindly following guides to get results that seems similar.

One final note, this week's lecture and learning diary improved my overall understanding of the challenges with remote sensing data, but I'm glad that the processing is built into existing packages or through Analysis Ready Data.
