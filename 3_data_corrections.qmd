# Remote Sensing Data and Corrections {#remote-sensing-data-corrections}

## Summary {#sec-3-summary}

This week we delved deeper into remote sensing topics particularly data corrections, joining and enhancements. Lots of new terms were raised and some of the concepts were challenging to understand so it's easiest to summarise these terms by building a @tbl-remote-sensing-glossary. A lot of these terms could have entire lectures dedicated to them so I attempt to put these into a simple terms as possible.

*sources need to be added to the caption below*

```{r include=FALSE}
library(tidyverse)
library(knitr)
library(here)
library(kableExtra)

#read in data
glos <- readxl::read_excel(here("tables", "rs_glossary.xlsx"))%>% 
  knitr::kable(booktabs = TRUE)%>% 
  kable_styling(position = "center", full_width = T)%>%
  # any specifc row changes you want
    row_spec(.,
  row=0,
  bold = TRUE)
```

```{r echo=FALSE}
#| label: tbl-remote-sensing-glossary
#| tbl-cap: "Glossary of remote sensing terminology. **Sources: **"
glos
```

A key point raised throughout the lecture was that regression plays a key part in many of the corrections that we looked into. This links back to (@sec-1-application) discussion and the regression based methods used by @mandanici2016. I initially struggled to fully understand this methodology fully so this lecture helped to consolidate those comparisons by clearly explaining the important role that regression plays in remote sensing.

Once again we have near endless options to analyse further but this week I will focus on atmospheric correction by comparing against radiometric normalisation. This specifically looks at the differences and accuracy of **relative** and **absolute** correction.

## Applications - Effectiveness of relative and absolute correction {#sec-3-application}

The is an interesting area to investigate is because the availability of the inputs required for absolute atmospheric correction is often generalised over an specified area or season potentially leading to accuracy issues. Both @hu2011 and @bernardo2016 investigation the impact of this generalisation, whilst there are differences between the two methodologies which will be discussed later but first lets look at the similarities:

-   compares using Landsat imagery
-   compares a single study area using time series images
-   uses FLAASH for absolute atmospheric correction
-   uses multivariate alternative detection (MAD) for relative radiometric normalisation

### Absolute Vs Relative methods and results

@bernardo2016 investigated the reflectance errors comparing by assessing root mean squared error (RMSE) and the mean absolute percent error (MAPE). A comparison was made against the FLAASH (absolute method) against IRMAD - iteratively reweighted multivariate alteration detection (relative method) correction methods. The results in @tbl-reflectance-errors clearly show greater levels in inaccuracy from the absolute method (FLAASH) in both methods of comparison across nearly all bands.

```{r include=FALSE}
#read in data
referrors <- read_csv(here("tables","flaash_irmad_comparison.csv"), locale = locale(encoding = "UTF-8"))%>% 
  knitr::kable(booktabs = TRUE)%>% 
  kable_styling(position = "center", full_width = T)%>%
    column_spec(c(2,3), background = "#F5F5F5") %>%
    column_spec(c(4,5), background = "#D3D3D3") %>%
    row_spec(., row=0, bold = TRUE)
```

```{r echo=FALSE}
#| label: tbl-reflectance-errors
#| tbl-cap: "Reflectance errors from FLAASH and IRMAD corrections **[@bernardo2016]**"
referrors
```

@hu2011 used MAD, over @bernardo2016's technique of IRMAD, which provides a more basic detection of change as it is a one-pass approach and does not iteratively refine the results, also IRMAD are much less noisy than MAD(@nielsen2005, @nielsen2007). There is also a third approach, regularised IRMAD which can perform even better but this was not part of the studies being compared.

@hu2011 used regression to assess the difference between corrections and the findings can be reviewed through the standard deviation results of each method. Generally the results are comparable to @bernardo2016 with both methods reducing radiometric distortion, however relative normalisation corrects it more effectively. This is shown by the lower standard deviations of relative normalisation versus atmospheric correction across all bands (@tbl-std-comparison).

```{r include=FALSE}
#read in data
sdcomp <- read_csv(here("tables","sd_rel_ab_comparison.csv"), locale = locale(encoding = "UTF-8"))%>% 
  knitr::kable(booktabs = TRUE)%>% 
  kable_styling(position = "center", full_width = T)%>%
    row_spec(., row=0, bold = TRUE)
```

```{r echo=FALSE}
#| label: tbl-std-comparison
#| tbl-cap: "Standard Deviation of the difference in each band for relative normalisation against atmospheric correction"
sdcomp
```

### Review

Whilst the papers talk about absolute corrections having high percentage error in certain bands this could be specific to the location of the study and any assumptions made throughout. There are so many variables that it would be highly improbable to produce a study that could feasibly review all of these aspects. Ideally it would be possible to say exactly what corrections/enhancements to apply to any image to provide the optimal output. However that is just not possible, considering variables including atmospheric conditions, topological factors, sensor differences, then we can begin to see why there is no single set of tools for analysis.

This is evident in both papers and also seen in other papers such as @mandanici2016 and @nasiri2022. It could appear that the scope is quite limited as they focus on very small samples for their analysis (previously discussed in week 1). However if we consider this from a technical point of view then simply the amount of computational power required to process much larger samples is very restrictive and time-consuming.

## Reflection {#sec-3-refection}

Before reflecting on some of the more technical aspects discussed I think it's first important to highlight the role that Virginia Norwood played in modern remote sensing. This was covered right at the beginning of the lecture and is quite an interesting story. A whole review could written about this but I think the fact the Virginia Norwood is known as the "mother of Landsat" says a great deal, and the full story of her life can be found in multiple articles including @mcclain2023 and @inventorshalloffame2025.

So back to the reflection, this week was definitely challenging, particularly with the amount of different terms, ideas and formulas raised. However when I really began to delve deeper into these concepts and particularly those discussed in @sec-3-application they really become clearer. You can really start to see the benefit these corrections and enhancements can have as part of wider studies.

As mentioned previously I'm looking forward to putting this ideas into practice through the Earth Observation Data Hub pilot scheme that Camden will soon have access to. I want to explore what data we will get access to and can finally understand the differences between products and particular what information can be derived from different bands. This will be a big improvement in the previous EO work that I've done which involved blindly following guides to try to get a result that seems similar.

One final note, even though it's good to understand the processing required and it really helped boost my overall understanding of the challenges with remote sensing data, I'm so glad that most of the processing is already built into packages or in the Analysis Ready Data.
