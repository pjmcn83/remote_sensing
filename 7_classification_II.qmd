# Classification II

## Summary {#sec-7-summary}

As we continued exploring classification we this time spoke about the use of *pre-classified* over *self-classified* data.

With a wealth of preclassified datasets available it would be very easy to pick these up and use these for the basis of any studies being undertaken. However, if we do not consider the processes used, the accuracy of the data (not just taking the stated accuracy as gospel) and the use cases of the product then we open ourselves up to misinterpretation of both the product and of our own research. In the lecture we discussed [Dynamic World's](https://www.dynamicworld.app/explore/) global 10m resolution near real-time land cover dataset which sounds amazing at face value but delving into the methodology there are concerning details such as the use of Top Of Atmosphere (TOA) data for training which means results could be skewed by atmospheric gases. There is also the fact that we do not control the parameters and outputs of the pre-classified data which makes it less than ideal to use. Specifically would [MODIS]{https://modis.gsfc.nasa.gov/data/dataprod/mod12.php} 500m land cover data actually be useful to investigate a specific urban area, or would [ESA's World Cover](https://viewer.esa-worldcover.org/worldcover/) product be suitable for recent analysis when it only covers 2020 and 2021.

We can see that while pre-classified may be a good point of reference it is highly unlikely to be suitable to more specific use cases. This is before we have even considered the accuracy of the products on offer. To effectively use any data including our own self-classified data we must look at its accuracy to ensure it is fit for purpose. This can become quite complex and how is the accuracy of these techniques measured and what could be the specific issues related to these?

## Application {#sec-7-application}

Often readers will take the output of an analysis such as a map, chart or diagram as the truth just because it has been published, however the accuracy of such work should also be considered even if it is just for the researcher/analyst's own piece of mind.

This is doubly true as it helps prove that the techniques used for analysis are appropriate for the use case. There is no one correct way to approach classification therefore assessing the accuracy must also be considered in our study. In classification we can use accuracy assessment to legitimise our work but must also be careful that limitations also apply in these assessments.

@foody2002 states that the confusion matrix is key to much work on accuracy assessment but is often used without questioning it's suitability. Foody continues that data used for accuracy assessments will never be completely site-specific due to issues related to misrepresentation of the ground both in remote sensing data and through over generalisation of specified classes. This is something that has always concerned me as I am often questioning myself whether my training data is too specific or generalised. This is something that I would consider exploring the effects of to truly consolidate my own interpretation.

Whilst there is no perfect solution for accuracy assessment @olofsson2014 suggested some "good practice" in relation to assessing land change. They summarise that without an accuracy assessment it is not possible to communicate map quality in a meaningful way and state the building blocks of producing effective and high scoring accuracy assessment. Of particular interest is the step related to use of reference data and acknowledging the uncertainty of this "gold standard" dataset. This links back to the Dynamic World example in @sec-7-summary where TOA data was being used for training data when there are surely better reference datasets available even if they are harder to access or process.

## Reflection {#sec-7-reflection}

Once aspect of classification that I'm aware that I haven't mentioned is the spatial autocorrelation of our data. We must be conscious of the first law of geography stating "everything is related to everything else, but near things are more related than distant things." Meaning that if our samples are too close together then accuracy is likely to be high but the result will be poorer when applied across a wider area, so be aware of *overfitting* a model. This is yet another thing to consider in any accuracy assessment or when interpreting someone else's work. **Don't just look at accuracy figures** also consider the steps taken to get there and consider any limitations in that methodology or the data used.



